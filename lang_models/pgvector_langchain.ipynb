{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf61df5",
   "metadata": {},
   "source": [
    "# Building AI-powered apps on Google Cloud databases using pgvector, LLMs and LangChain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Pandas dataframe in a PostgreSQL table.\n",
    "import asyncio\n",
    "import asyncpg\n",
    "from google.cloud.sql.connector import Connector\n",
    "\n",
    "async def main(df, tbl_name):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\"\n",
    "        )\n",
    "\n",
    "        # Create the table. \n",
    "        await conn.execute(\"\"\"CREATE TABLE products(\n",
    "                                product_id VARCHAR(1024) PRIMARY KEY,\n",
    "                                product_name TEXT,\n",
    "                                description TEXT,\n",
    "                                list_price NUMERIC)\"\"\")\n",
    "\n",
    "        # Copy the dataframe to the `products` table.\n",
    "        await df.to_sql(f'{tbl_name}', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "# Run the SQL commands now.\n",
    "await main() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82629708",
   "metadata": {},
   "source": [
    "# Generating the vector embeddings using Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48ae48",
   "metadata": {},
   "source": [
    "# # Split long text into smaller chunks with LangChain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks(df):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators = [\".\", \"\\n\"],\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap  = 0,\n",
    "        length_function = len,\n",
    "    )\n",
    "    chunked = []\n",
    "    for index, row in df.iterrows():\n",
    "        product_id = row['product_id']\n",
    "        desc = row['description']\n",
    "        splits = text_splitter.create_documents([desc])\n",
    "        for s in splits:\n",
    "            r = { 'product_id': product_id, 'content': s.page_content }\n",
    "            chunked.append(r)\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d887dc",
   "metadata": {},
   "source": [
    "# # Get the vector embedding using Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b003e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "def get_embedding():\n",
    "    aiplatform.init(project=f\"{project_id}\", location=f\"{region}\")\n",
    "    embeddings_service = VertexAIEmbeddings()\n",
    "\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(chunked), batch_size):\n",
    "        request = [x['content'] for x in chunked[i: i + batch_size]]\n",
    "        response = embeddings_service.embed_documents(request)\n",
    "    # Post-process the generated embeddings.\n",
    "    return embeddings_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3a878",
   "metadata": {},
   "source": [
    "# # Use pgvector to store the generated embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00544427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgvector.asyncpg import register_vector\n",
    "\n",
    "# ...\n",
    "await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "await register_vector(conn)\n",
    "\n",
    "\n",
    "# Create the `product_embeddings` table to store vector embeddings.\n",
    "await conn.execute(\"\"\"CREATE TABLE product_embeddings(\n",
    "                             product_id VARCHAR(1024), \n",
    "                             content TEXT,\n",
    "                             embedding vector(768))\"\"\")\n",
    "\n",
    "\n",
    "# Store all the generated embeddings.\n",
    "for index, row in product_embeddings.iterrows():\n",
    "    await conn.execute(\"INSERT INTO product_embeddings VALUES ($1, $2, $3)\"                                    \n",
    "                       row['product_id'], \n",
    "                       row['content'], \n",
    "                       np.array(row['embedding']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e853ab2",
   "metadata": {},
   "source": [
    "# Finding similar toys using pgvector cosine search operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd80b69",
   "metadata": {},
   "source": [
    "# # Step 1: Generate the vector embedding for the incoming input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate vector embedding for the user query.\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "embeddings_service = VertexAIEmbeddings()\n",
    "qe = embeddings_service.embed_query([user_query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d0966",
   "metadata": {},
   "source": [
    "# # Step 2: Use the new pgvector cosine similarity search operator to find related products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cosine similarity search to find the top five products \n",
    "# that are most closely related to the input query.\n",
    "\n",
    "results = await conn.fetch(\"\"\"\n",
    "             WITH vector_matches AS (\n",
    "                     SELECT product_id, \n",
    "                            1 - (embedding <=> $1) AS similarity\n",
    "                     FROM product_embeddings\n",
    "                     WHERE 1 - (embedding <=> $1) > $2\n",
    "                     ORDER BY similarity DESC\n",
    "                     LIMIT $3\n",
    "             )\n",
    "             SELECT product_name, \n",
    "                    list_price, \n",
    "                    description \n",
    "             FROM products\n",
    "             WHERE product_id IN (SELECT product_id FROM vector_matches)\n",
    "                   AND list_price >= $4 AND list_price <= $5\n",
    "             \"\"\", \n",
    "             qe, similarity_threshold, num_matches, min_price, max_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f1b70",
   "metadata": {},
   "source": [
    "# Use case 1: Building an AI-curated contextual hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab323bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LangChain for summarization and efficient context building.\n",
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "llm = VertexAI()\n",
    "\n",
    "map_prompt_template = \"\"\"\n",
    "              You will be given a detailed description of a toy product.\n",
    "              This description is enclosed in triple backticks (```).\n",
    "              Using this description only, extract the name of the toy,\n",
    "              the price of the toy and its features.\n",
    "\n",
    "              ```{text}```\n",
    "              SUMMARY:\n",
    "              \"\"\"\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                You will be given a detailed description different toy products\n",
    "                enclosed in triple backticks (```) and a question enclosed in\n",
    "                double backticks(``).\n",
    "                Select one toy that is most relevant to answer the question.\n",
    "                Using that selected toy description, answer the following\n",
    "                question in as much detail as possible.\n",
    "                You should only use the information in the description.\n",
    "                Your answer should include the name of the toy, the price of\n",
    "                the toy and its features. \n",
    "                Your answer should be less than 200 words.\n",
    "                Your answer should be in Markdown in a numbered list format.\n",
    "\n",
    "\n",
    "                Description:\n",
    "                ```{text}```\n",
    "\n",
    "\n",
    "                Question:\n",
    "                ``{user_query}``\n",
    "\n",
    "\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "combine_prompt = PromptTemplate(template=combine_prompt_template, input_variables=[\"text\", \"user_query\"])\n",
    "\n",
    "docs = [Document(page_content=t) for t in matches]\n",
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"map_reduce\",\n",
    "                             map_prompt=map_prompt,\n",
    "                             combine_prompt=combine_prompt)\n",
    "answer = chain.run({\n",
    "                      'input_documents': docs,\n",
    "                      'user_query': user_query,\n",
    "                    })\n",
    "\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abef12d",
   "metadata": {},
   "source": [
    "# Use case 2: Adding AI-powered creative content generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18195775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "template = \"\"\"\n",
    "            You are given descriptions about some similar kind of toys in the\n",
    "            context. This context is enclosed in triple backticks (```).\n",
    "            Combine these descriptions and adapt them to match the \n",
    "            specifications in the initial prompt. \n",
    "            All the information from the initial prompt must be included. \n",
    "            You are allowed to be as creative as possible,\n",
    "            Describe the new toy in as much detail. Your answer should be\n",
    "            less than 200 words.\n",
    "\n",
    "            Context:\n",
    "            ```{context}```\n",
    "\n",
    "            Initial Prompt:\n",
    "            {creative_prompt}\n",
    "\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"creative_prompt\"])\n",
    "\n",
    "# Increase the `temperature` to allow more creative writing freedom.\n",
    "llm = VertexAI(temperature=0.7)\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "answer = llm_chain.run({\n",
    "    \"context\": '\\n'.join(matches),\n",
    "    \"creative_prompt\": creative_prompt,\n",
    "    })\n",
    "\n",
    "\n",
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
