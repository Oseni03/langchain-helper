{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4388ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI \n",
    "from langchain.memory import PostgresChatMessageHistory\n",
    "from langchain.chains import OpenAIModerationChain, SequentialChain, LLMChain, SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b309515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModeration(OpenAIModerationChain):\n",
    "    \"\"\"\n",
    "    Moderation chains are useful for \n",
    "    detecting text that could be hateful, \n",
    "    violent, etc. This can be useful to \n",
    "    apply on both user input, but also on \n",
    "    the output of a Language Model.\n",
    "    \n",
    "    Here's an example of creating a custom \n",
    "    moderation chain with a custom error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _moderate(self, text: str, results: dict) -> str:\n",
    "        if results[\"flagged\"]:\n",
    "            error_str = f\"The following text was found that violates OpenAI's content policy: {text}\"\n",
    "            return error_str\n",
    "        return text\n",
    "    \n",
    "    moderation_chain = OpenAIModerationChain(error=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b64c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocBasicQA:\n",
    "    def __init__(self, file_path):\n",
    "        self.postgres_uri = \"postgresql://postgres:mypassword@localhost/chat_history\"\n",
    "        self.postgres_session_id = \"foo\"\n",
    "        self.texts = self.get_text(file_path)\n",
    "        \n",
    "        # Moderation check\n",
    "        self.moderation_chain = OpenAIModerationChain(error=True)\n",
    "    \n",
    "    def get_text(self, file_path):\n",
    "        with open(file_path) as f:\n",
    "            state_of_the_union = f.read()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        texts = text_splitter.split_text(state_of_the_union)\n",
    "        return text \n",
    "    \n",
    "    def get_relevant_doc(self, text, query):\n",
    "        # For QA with sources \n",
    "        # return docsearch.similarity_search(query)\n",
    "        \n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()\n",
    "        return docsearch.get_relevant_documents(query)\n",
    "    \n",
    "    def get_output(self, rel_doc, query):\n",
    "        # For QA with sources \n",
    "        # chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\")\n",
    "        chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\")\n",
    "        output = chain({\"input_documents\": rel_doc, \"question\": query}, return_only_outputs=True)\n",
    "        return output[\"output_text\"]\n",
    "    \n",
    "    def get_output__custom_temp(self, **kwargs):\n",
    "        \"\"\"\n",
    "        This generate a reply from a custom prompt templates (question_prompt, combine_prompt)\n",
    "        \n",
    "        parameters\n",
    "            qn_prompt_template -> text \n",
    "            qn_prompt_variables -> variables for qn_prompt_template\n",
    "            \n",
    "            cb_prompt_template -> text \n",
    "            cb_prompt_variables -> variables for cb_prompt_template\n",
    "            \n",
    "            rel_doc -> get_relevant_doc instance\n",
    "            query -> text query\n",
    "        \"\"\"\n",
    "        QUESTION_PROMPT = PromptTemplate(\n",
    "            template=kwargs[\"qn_prompt_template\"], \n",
    "            input_variables=kwargs[\"qn_prompt_variables\"]\n",
    "        )\n",
    "        \n",
    "        COMBINE_PROMPT = PromptTemplate(\n",
    "            template=kwargs[\"cb_prompt_template\"],\n",
    "            input_variables=kwargs[\"cb_prompt_variables\"]\n",
    "        )\n",
    "        # For QA with sources \n",
    "        # chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
    "        chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
    "        output = chain({\n",
    "            \"input_documents\": kwargs[\"rel_doc\"], \n",
    "            \"question\": kwargs[\"query\"]\n",
    "        }, return_only_outputs=True)\n",
    "        return output[\"output_text\"]\n",
    "    \n",
    "    def save_message(self, user_msg, ai_msg):\n",
    "        history = PostgresChatMessageHistory(\n",
    "            connection_string=self.postgres_uri,\n",
    "            session_id=self.postgres_session_id,\n",
    "        )\n",
    "        \n",
    "        history.add_user_message(user_msg)\n",
    "        history.add_ai_message(ai_msg)\n",
    "        return history.messages\n",
    "    \n",
    "    def run(query, **kwargs):\n",
    "        \"\"\"\n",
    "        keywords parameters\n",
    "            qn_prompt_template -> text \n",
    "            qn_prompt_variables -> variables for qn_prompt_template\n",
    "            \n",
    "            cb_prompt_template -> text \n",
    "            cb_prompt_variables -> variables for cb_prompt_template\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            query = self.moderation_chain.run(query)\n",
    "        except ValueError:\n",
    "            return f\"Text was found that violates OpenAI's content policy\"\n",
    "        \n",
    "        rel_doc = self.get_relevant_doc(self.texts, query)\n",
    "        \n",
    "        if kwargs:\n",
    "            kwargs[\"rel_doc\"] = rel_doc\n",
    "            kwargs[\"query\"] = query\n",
    "            output = self.get_output__custom_temp(kwargs)\n",
    "        else:\n",
    "            output = self.get_output(rel_doc, query)\n",
    "        \n",
    "        self.save_message(query, output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocRetrieverQA:\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "\n",
    "        self.postgres_uri = \"postgresql://postgres:mypassword@localhost/chat_history\"\n",
    "\n",
    "        self.postgres_session_id = \"foo\"\n",
    "\n",
    "        self.text = self.get_text(file_path)\n",
    "\n",
    "        # Moderation check\n",
    "\n",
    "        self.moderation_chain = OpenAIModerationChain(error=True)\n",
    "\n",
    "\n",
    "\n",
    "    def get_text(self, file_path):\n",
    "\n",
    "        with open(file_path) as f:\n",
    "\n",
    "            state_of_the_union = f.read()\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "        texts = text_splitter.split_text(state_of_the_union)\n",
    "\n",
    "        return text \n",
    "\n",
    "    \n",
    "\n",
    "    def save_message(self, user_msg, ai_msg):\n",
    "\n",
    "        history = PostgresChatMessageHistory(\n",
    "\n",
    "            connection_string=self.postgres_uri,\n",
    "\n",
    "            session_id=self.postgres_session_id,\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        history.add_user_message(user_msg)\n",
    "\n",
    "        history.add_ai_message(ai_msg)\n",
    "\n",
    "        return history.messages\n",
    "    \n",
    "    def run(query):\n",
    "\n",
    "        try:\n",
    "\n",
    "            query = self.moderation_chain.run(query)\n",
    "\n",
    "        except ValueError:\n",
    "\n",
    "            return f\"Text was found that violates OpenAI's content policy\"\n",
    "\n",
    "        \n",
    "\n",
    "        for i, text in enumerate(self.texts):\n",
    "\n",
    "            text.metadata['source'] = f\"{i}-pl\"\n",
    "\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "\n",
    "        docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "        \n",
    "\n",
    "        llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "\n",
    "        \n",
    "\n",
    "        qa_chain = create_qa_with_sources_chain(llm)\n",
    "\n",
    "        \n",
    "\n",
    "        doc_prompt = PromptTemplate(\n",
    "\n",
    "            template=\"Content: {page_content}\\nSource: {source}\",\n",
    "\n",
    "            input_variables=[\"page_content\", \"source\"],\n",
    "\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        final_qa_chain = StuffDocumentsChain(\n",
    "\n",
    "            llm_chain=qa_chain, \n",
    "\n",
    "            document_variable_name='context',\n",
    "\n",
    "            document_prompt=doc_prompt,\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        retrieval_qa = RetrievalQA(\n",
    "\n",
    "            retriever=docsearch.as_retriever(),\n",
    "\n",
    "            combine_documents_chain=final_qa_chain\n",
    "\n",
    "        )\n",
    "\n",
    "        output = retrieval_qa.run(query)\n",
    "\n",
    "        self.save_message(query, output)\n",
    "\n",
    "        return output \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
